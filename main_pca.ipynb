{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.embedding_visualizer_service import EmbeddingVisualizer\n",
    "import pandas as pd     \n",
    "\n",
    "# Read LLM answers\n",
    "true_answers = pd.read_csv('./data/llm_responses/NQ-llm-true-answers_llama3.1.csv')\n",
    "correct_answers = pd.read_csv('./data/llm_responses/NQ-llm-correct_answers_llama3.1.csv')\n",
    "factual_inaccuracy = pd.read_csv('./data/llm_responses/NQ-llm-factual_inaccuracy_llama3.1.csv')\n",
    "needle_in_haystack = pd.read_csv('./data/llm_responses/NQ-llm-needle_in_haystack_llama3.1.csv')\n",
    "\n",
    "# Rename columns and concatenate datasets, keeping only the required columns\n",
    "df_combined = pd.concat([\n",
    "    true_answers.rename(columns={'true_answer_embedding_llama3.1': 'NQ TRUE answers llama3.1'})[['NQ TRUE answers llama3.1']],\n",
    "    correct_answers.rename(columns={'embedding_llama3.1': 'correct_answers_llama3.1'})[['correct_answers_llama3.1']],\n",
    "    factual_inaccuracy.rename(columns={'embedding_llama3.1': 'factual_inaccuracy_llama3.1'})[['factual_inaccuracy_llama3.1']],\n",
    "    needle_in_haystack.rename(columns={'embedding_llama3.1': 'needle_in_haystack_llama3.1'})[['needle_in_haystack_llama3.1']]\n",
    "], axis=1)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "# print(df_combined)\n",
    "\n",
    "vis = EmbeddingVisualizer(df=df_combined)\n",
    "vis.convert_columns_to_float_arrays()\n",
    "df_pca = vis.process_embeddings_PCA(target_dim=2, df=df_combined)\n",
    "vis.plot_umap_2d(df=df_pca, title=\"Natural Questions PCA. True Answers VS LLM Hallucinations\", save_path='./results_imgs/umap_2d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = EmbeddingVisualizer(df=df_combined)\n",
    "vis.convert_columns_to_float_arrays()\n",
    "vis.process_embeddings_padding()\n",
    "vis.apply_umap(n_components=3)\n",
    "vis.plot_umap_3d(title=\"NQ correct answers VS LLM answers\", save_path='./results_imgs/umap_3d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = EmbeddingVisualizer(df=df_combined)\n",
    "vis.convert_columns_to_float_arrays()\n",
    "vis.process_embeddings_padding()\n",
    "vis.apply_umap(n_components=12)\n",
    "vis.plot_umap_12d_4x3d(title=\"NQ correct answers VS LLM answers\", save_path='./results_imgs/umap_12d_4x3d.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
